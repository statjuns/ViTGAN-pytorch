{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ViTGAN-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiVyqRSgDTpU"
      },
      "source": [
        "# ViTGAN pytorch implementation\n",
        "\n",
        "This notebook is a pytorch implementation of [VITGAN: Training GANs with Vision Transformers](https://arxiv.org/pdf/2107.04589v1.pdf)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1kJJw6BYW01HgooCZ2zUDt54e1mXqITXH?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjz0JmQyD4JJ"
      },
      "source": [
        "The model consists of a Vision Transformer Generator and a Vision Transformer Discriminator.\n",
        "\n",
        "It is adversarially trained to map latent vectors to images, which closely resemble the images from a given dataset. In this implementation, the dataset used is CIFAR-10.\n",
        "\n",
        "The Generator takes latent values $z$ as input, which is integrated in a Vision Transformer Encoder. The output for each patch of the image is fed to a SIREN network, in combination with a Fourier Embedding ($E_{fou}$)\n",
        "\n",
        "![ViTGAN Generator architecture](https://drive.google.com/uc?export=view&id=1XaCVOLq8Bvg-I3qM-bugNZcjIW5L7XTO)\n",
        "\n",
        "This implementation separates the Generator in Vision Transformer and SIREN networks for debugging purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZeCyoxmxwud"
      },
      "source": [
        "1.   [x] Use vectorized L2 distance in attention for **Discriminator**\n",
        "2.   [x] Overlapping Image Patches\n",
        "2.   [x] DiffAugment\n",
        "3.   [x] Self-modulated LayerNorm (SLN)\n",
        "4.   [x] Implicit Neural Representation for Patch Generation\n",
        "5.   [x] ExponentialMovingAverage (EMA)\n",
        "6.   [x] Balanced Consistency Regularization (bCR)\n",
        "7.   [x] Improved Spectral Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_gqyo3DZA3J"
      },
      "source": [
        "! pip install einops\n",
        "! pip install git+https://github.com/fadel/pytorch_ema\n",
        "! pip install stylegan2-pytorch\n",
        "! pip install tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT_rw6-yB6hG"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXCe_q68481Q"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn import Parameter\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
        "import numpy as np\n",
        "import skimage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from torch_ema import ExponentialMovingAverage\n",
        "\n",
        "from stylegan2_pytorch import stylegan2_pytorch\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-JOToTeB9uE"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeQq8IuCAPmZ"
      },
      "source": [
        "image_size = 32\n",
        "patch_size = 4\n",
        "latent_dim = 32 # Size of z\n",
        "hidden_features = 384\n",
        "depth = 4\n",
        "num_heads = 4\n",
        "\n",
        "combine_patch_embeddings = True # Generate an image from a single SIREN, instead of patch-by-patch\n",
        "combine_patch_embeddings_size = hidden_features\n",
        "\n",
        "sln_paremeter_size = hidden_features # either hidden_features or 1\n",
        "\n",
        "batch_size = 128\n",
        "device = \"cuda\"\n",
        "out_features = 3 # The number of color channels\n",
        "\n",
        "discriminator_type = \"stylegan2\" # \"vitgan\", \"cnn\", \"stylegan2\"\n",
        "\n",
        "lr = 3e-5 # Learning rate\n",
        "beta = (0, 0.99) # Adam oprimizer parameters for both the generator and the discriminator\n",
        "batch_size_history_discriminator = True # Whether to use a loss, which tracks one sample from last batch_size number of batches\n",
        "epochs = 200 # Number of epochs\n",
        "lambda_bCR_real = 10\n",
        "lambda_bCR_fake = 10\n",
        "lambda_lossD_noise = 0\n",
        "lambda_lossD_history = 0\n",
        "\n",
        "writer = SummaryWriter(log_dir=f'runs/patch_size-{patch_size}_\\\n",
        "hidden_features-{hidden_features}_combine_patch_embeddings-{combine_patch_embeddings}_\\\n",
        "sln_paremeter_size-{sln_paremeter_size}_discriminator_type-{discriminator_type}_\\\n",
        "num_heads-num_heads_\\\n",
        "depth-{depth}')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3dGermsa1bG"
      },
      "source": [
        "if combine_patch_embeddings:\n",
        "    out_patch_size = image_size\n",
        "    combined_embedding_size = combine_patch_embeddings_size\n",
        "else:\n",
        "    out_patch_size = patch_size\n",
        "    combined_embedding_size = hidden_features\n",
        "\n",
        "siren_in_features = combined_embedding_size\n",
        "\n",
        "siren_mapping_size = int(siren_in_features // 2)\n",
        "B_gauss = torch.randn((siren_mapping_size, 2)) * 10"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPmNZO9bAb2f"
      },
      "source": [
        "def get_mgrid(sidelen, dim=2):\n",
        "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.\n",
        "    sidelen: int\n",
        "    dim: int'''\n",
        "    tensors = tuple(dim * [torch.linspace(-1, 1, steps=sidelen)])\n",
        "    mgrid = torch.stack(torch.meshgrid(*tensors), dim=-1)\n",
        "    mgrid = mgrid.reshape(-1, dim)\n",
        "    return mgrid"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M4De21AB8lE"
      },
      "source": [
        "\n",
        "\n",
        "https://github.com/mit-han-lab/data-efficient-gans/blob/master/DiffAugment-stylegan2-pytorch/DiffAugment_pytorch.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsaytJSj0inJ"
      },
      "source": [
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.1):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2).contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.3):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA-uwpp7b1QK",
        "outputId": "d8f9a6c5-b483-45d0-9ccd-d8be172ebc20"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63v16amHIG2L"
      },
      "source": [
        "Visualize the effects of the DiffAugment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "fnQW4nBraOJb",
        "outputId": "2c157f73-de05-41fa-e660-0399f70d5928"
      },
      "source": [
        "img = next(iter(trainloader))[0]\n",
        "img = DiffAugment(img, policy='color,translation,cutout', channels_first=True)\n",
        "img = img.permute(0,2,3,1)[0]\n",
        "img -= img.min()\n",
        "img /= img.max()\n",
        "plt.imshow(img)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcbb76e45d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QKaT8pk9vD_"
      },
      "source": [
        "# Improved Spectral Normalization (ISN)\n",
        "\n",
        "$$\n",
        "\\bar{W}_{ISN}(W):=\\sigma(W_{init})\\cdot W/\\sigma(W)\n",
        "$$\n",
        "\n",
        "Reference code: https://github.com/koshian2/SNGAN\n",
        "\n",
        "When updating the weights, normalize the weights' norm to its norm at initialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL2ZLEnM-xct"
      },
      "source": [
        "def l2normalize(v, eps=1e-4):\n",
        "\treturn v / (v.norm() + eps)\n",
        "\n",
        "class spectral_norm(nn.Module):\n",
        "\tdef __init__(self, module, name='weight', power_iterations=1):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.module = module\n",
        "\t\tself.name = name\n",
        "\t\tself.power_iterations = power_iterations\n",
        "\t\tif not self._made_params():\n",
        "\t\t\tself._make_params()\n",
        "\t\tself.w_init_sigma = None\n",
        "\n",
        "\tdef _update_u_v(self):\n",
        "\t\tu = getattr(self.module, self.name + \"_u\")\n",
        "\t\tv = getattr(self.module, self.name + \"_v\")\n",
        "\t\tw = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "\t\theight = w.data.shape[0]\n",
        "\t\t_w = w.view(height, -1)\n",
        "\t\tfor _ in range(self.power_iterations):\n",
        "\t\t\tv = l2normalize(torch.matmul(_w.t(), u))\n",
        "\t\t\tu = l2normalize(torch.matmul(_w, v))\n",
        "\n",
        "\t\tsigma = u.dot((_w).mv(v))\n",
        "\t\tif type(self.w_init_sigma) != np.ndarray:\n",
        "\t\t\tself.w_init_sigma = np.array(sigma.expand_as(w).detach().cpu())\n",
        "\t\tsetattr(self.module, self.name, torch.tensor(self.w_init_sigma).to(device) * w / sigma.expand_as(w))\n",
        "\n",
        "\tdef _made_params(self):\n",
        "\t\ttry:\n",
        "\t\t\tgetattr(self.module, self.name + \"_u\")\n",
        "\t\t\tgetattr(self.module, self.name + \"_v\")\n",
        "\t\t\tgetattr(self.module, self.name + \"_bar\")\n",
        "\t\t\treturn True\n",
        "\t\texcept AttributeError:\n",
        "\t\t\treturn False\n",
        "\n",
        "\tdef _make_params(self):\n",
        "\t\tw = getattr(self.module, self.name)\n",
        "\n",
        "\t\theight = w.data.shape[0]\n",
        "\t\twidth = w.view(height, -1).data.shape[1]\n",
        "\n",
        "\t\tu = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "\t\tv = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "\t\tu.data = l2normalize(u.data)\n",
        "\t\tv.data = l2normalize(v.data)\n",
        "\t\tw_bar = Parameter(w.data)\n",
        "\n",
        "\t\tdel self.module._parameters[self.name]\n",
        "\t\tself.module.register_parameter(self.name + \"_u\", u)\n",
        "\t\tself.module.register_parameter(self.name + \"_v\", v)\n",
        "\t\tself.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "\tdef forward(self, *args):\n",
        "\t\tself._update_u_v()\n",
        "\t\treturn self.module.forward(*args)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttk-7hLuIZ4o"
      },
      "source": [
        "Vision Transformer reference code: \\[[Blog Post](https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632)\\]\n",
        "\n",
        "Normal Attention Mechanism\n",
        "\n",
        "$$\n",
        "Attention_h(X) = softmax \\bigg ( \\frac{QK^T}{\\sqrt{d_h}} V \\bigg )\n",
        "$$\n",
        "\n",
        "Lipschitz Attention Mechanism\n",
        "\n",
        "$$\n",
        "Attention_h(X) = softmax \\bigg ( \\frac{d(Q,K)}{\\sqrt{d_h}} V \\bigg )\n",
        "$$\n",
        "\n",
        "where $d(Q,K)$ is L2-distance.\n",
        "\n",
        "https://arxiv.org/pdf/2006.04710.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1VgcIu6MFow"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size=384, num_heads=4, dropout=0, discriminator=False):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.discriminator = discriminator\n",
        "        # fuse the queries, keys and values in one matrix\n",
        "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "        if self.discriminator:\n",
        "            self.qkv = spectral_norm(self.qkv)\n",
        "            self.projection = spectral_norm(self.projection)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        # split keys, queries and values in num_heads\n",
        "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
        "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
        "        if self.discriminator:\n",
        "            # calculate L2-distances\n",
        "            energy = torch.cdist(queries.contiguous(), keys.contiguous(), p=2)\n",
        "        else:\n",
        "            # sum up over the last axis\n",
        "            energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
        "\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "            \n",
        "        scaling = self.emb_size ** (1/2)\n",
        "        att = F.softmax(energy, dim=-1) / scaling\n",
        "        att = self.att_drop(att)\n",
        "        # sum up over the third axis\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpUuzMsS9G-t"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GJiP09nL_Lh"
      },
      "source": [
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, expansion=4, drop_p=0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4713MviFIl8R"
      },
      "source": [
        "Self-Modulated LayerNorm\n",
        "$$\n",
        "SLN(h_{\\ell},w)=\\gamma_{\\ell}(w)\\odot\\frac{h_{\\ell}-\\mu}{\\sigma}+\\beta_{\\ell}(w)\n",
        "$$\n",
        "\n",
        "where $\\gamma_{\\ell}, \\beta_{\\ell}\\in \\mathbb{R}^D$ or $\\gamma_{\\ell}, \\beta_{\\ell}\\in \\mathbb{R}^1$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdd9c_sk9GTC"
      },
      "source": [
        "class SLN(nn.Module):\n",
        "    def __init__(self, input_size, parameter_size=None):\n",
        "        super().__init__()\n",
        "        if parameter_size == None:\n",
        "            parameter_size = input_size\n",
        "        self.ln = nn.LayerNorm(input_size)\n",
        "        self.gamma = nn.Linear(input_size, parameter_size)\n",
        "        self.beta = nn.Linear(input_size, parameter_size)\n",
        "\n",
        "    def forward(self, hidden, w):\n",
        "        gamma = self.gamma(w).unsqueeze(1)\n",
        "        beta = self.beta(w).unsqueeze(1)\n",
        "        ln = self.ln(hidden)\n",
        "        return gamma * ln + beta"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvIYYkHe9J3f"
      },
      "source": [
        "class GeneratorTransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hidden_size=384,\n",
        "                 sln_paremeter_size=384,\n",
        "                 drop_p=0.,\n",
        "                 forward_expansion=4,\n",
        "                 forward_drop_p=0.,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "        self.sln = SLN(hidden_size, parameter_size=sln_paremeter_size)\n",
        "        self.msa = MultiHeadAttention(hidden_size, **kwargs)\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "        self.feed_forward = FeedForwardBlock(hidden_size, expansion=forward_expansion, drop_p=forward_drop_p)\n",
        "\n",
        "    def forward(self, hidden, w):\n",
        "        res = hidden\n",
        "        hidden = self.sln(hidden, w)\n",
        "        hidden = self.msa(hidden)\n",
        "        hidden = self.dropout(hidden)\n",
        "        hidden += res\n",
        "\n",
        "        res = hidden\n",
        "        hidden = self.sln(hidden, w)\n",
        "        self.feed_forward(hidden)\n",
        "        hidden = self.dropout(hidden)\n",
        "        hidden += res\n",
        "        return hidden"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXueAJqx9LsE"
      },
      "source": [
        "class GeneratorTransformerEncoder(nn.Module):\n",
        "    def __init__(self, depth=4, **kwargs):\n",
        "        self.depth = depth\n",
        "        self.blocks = [GeneratorTransformerEncoderBlock(**kwargs).to(device) for _ in range(depth)]\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, hidden, w):\n",
        "        for i in range(self.depth):\n",
        "            hidden = self.blocks[i](hidden, w)\n",
        "        return hidden"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ru0trNs9N3F"
      },
      "source": [
        "class GeneratorViT(nn.Module):\n",
        "    def __init__(self,\n",
        "                patch_size=4,\n",
        "                latent_dim=32,\n",
        "                hidden_size=384,\n",
        "                sln_paremeter_size=1,\n",
        "                image_size=32,\n",
        "                depth=4,\n",
        "                combine_patch_embeddings=False,\n",
        "                combined_embedding_size=1024,\n",
        "                **kwargs):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 384),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        num_patches = int(image_size//patch_size)**2\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "        self.image_size = image_size\n",
        "        self.combine_patch_embeddings = combine_patch_embeddings\n",
        "        self.combined_embedding_size = combined_embedding_size\n",
        "\n",
        "        self.pos_emb = nn.Parameter(torch.randn(num_patches, hidden_size))\n",
        "        self.transformer_encoder = GeneratorTransformerEncoder(depth, hidden_size=hidden_size, **kwargs)\n",
        "        self.sln = SLN(hidden_size, parameter_size=sln_paremeter_size).to(device)\n",
        "        self.to_single_emb = nn.Sequential(\n",
        "            nn.Linear(num_patches*hidden_size, combined_embedding_size),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        w = self.mlp(z)\n",
        "        pos = repeat(torch.sin(self.pos_emb), 'n e -> b n e', b=z.shape[0])\n",
        "        hidden = self.transformer_encoder(pos, w)\n",
        "\n",
        "        if self.combine_patch_embeddings:\n",
        "            # Output [batch_size, combined_embedding_size]\n",
        "            hidden = self.sln(hidden, w).view((z.shape[0], -1))\n",
        "            hidden = self.to_single_emb(hidden)\n",
        "        else:\n",
        "            # Output [batch_size*num_patches, hidden_size]\n",
        "            hidden = self.sln(hidden, w).view((-1, self.hidden_size))\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiQtj0Qa9b7L"
      },
      "source": [
        "# SIREN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH09oAfX481V"
      },
      "source": [
        "Code for SIREN is taken from [SIREN reference colab notebook](https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgjY_FpH481X"
      },
      "source": [
        "class SineLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True,\n",
        "                 is_first=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "        \n",
        "        self.in_features = in_features\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "        \n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                self.linear.weight.uniform_(-1 / self.in_features, \n",
        "                                             1 / self.in_features)      \n",
        "            else:\n",
        "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n",
        "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        return torch.sin(self.omega_0 * self.linear(input))\n",
        "    \n",
        "class Siren(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, hidden_layers, out_features, outermost_linear=False, \n",
        "                 first_omega_0=30, hidden_omega_0=30.):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.net = []\n",
        "        self.net.append(SineLayer(in_features, hidden_features, \n",
        "                                  is_first=True, omega_0=first_omega_0))\n",
        "\n",
        "        for i in range(hidden_layers):\n",
        "            self.net.append(SineLayer(hidden_features, hidden_features, \n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        if outermost_linear:\n",
        "            final_linear = nn.Linear(hidden_features, out_features)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0, \n",
        "                                              np.sqrt(6 / hidden_features) / hidden_omega_0)\n",
        "                \n",
        "            self.net.append(final_linear)\n",
        "        else:\n",
        "            self.net.append(SineLayer(hidden_features, out_features, \n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "        \n",
        "        self.net = nn.Sequential(*self.net)\n",
        "    \n",
        "    def forward(self, coords):\n",
        "        coords = coords.clone().detach().requires_grad_(True) # allows to take derivative w.r.t. input\n",
        "        output = self.net(coords)\n",
        "        return output, coords"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8DB73-TBGEs"
      },
      "source": [
        "$$\n",
        "Fou(\\mathbf{v})= \\left[ \\cos(2 \\pi \\mathbf B \\mathbf{v}), \\sin(2 \\pi \\mathbf B \\mathbf{v}) \\right]^\\mathrm{T}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixwQcQQaYBiD"
      },
      "source": [
        "def fourier_input_mapping(x):\n",
        "    x_proj = (2. * np.pi * x) @ B_gauss.t()\n",
        "    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
        "\n",
        "def fourier_pos_embedding():\n",
        "    # Create input pixel coordinates in the unit square\n",
        "    coords = np.linspace(-1, 1, out_patch_size, endpoint=True)\n",
        "    pos = np.stack(np.meshgrid(coords, coords), -1)\n",
        "    pos = torch.tensor(pos, dtype=torch.float)\n",
        "    result = fourier_input_mapping(pos).reshape([out_patch_size**2, siren_in_features])\n",
        "    return result.to(device)\n",
        "\n",
        "def mix_hidden_and_pos(hidden):\n",
        "    pos = fourier_pos_embedding()\n",
        "    hidden = repeat(hidden, 'n h -> n p h', p = out_patch_size**2)\n",
        "\n",
        "    # Combine the outputs from the Vision Transformer\n",
        "    # with the Fourier position embeddings\n",
        "    result = hidden + pos\n",
        "    \n",
        "    return result"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1Bz035Ugroo"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY_LoVXOgtao"
      },
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=4, stride_size=4, emb_size=384, image_size=32, batch_size=64):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.projection = nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains\n",
        "            spectral_norm(nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=stride_size)).to(device),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        num_patches = ((image_size-patch_size+stride_size) // stride_size) **2 + 1\n",
        "        self.positions = nn.Parameter(torch.randn(num_patches, emb_size))\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.projection(x)\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        # prepend the cls token to the input\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        # add position embedding\n",
        "        x += torch.sin(self.positions)\n",
        "        return x"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8DZy_pOg0ID"
      },
      "source": [
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        \n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkfQ73pSgwQo"
      },
      "source": [
        "class DiscriminatorTransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size=384,\n",
        "                 drop_p=0.,\n",
        "                 forward_expansion=4,\n",
        "                 forward_drop_p=0.,\n",
        "                 **kwargs):\n",
        "        super().__init__(\n",
        "                ResidualAdd(nn.Sequential(\n",
        "                    nn.LayerNorm(emb_size),\n",
        "                    MultiHeadAttention(emb_size, **kwargs),\n",
        "                    nn.Dropout(drop_p)\n",
        "                )),\n",
        "                ResidualAdd(nn.Sequential(\n",
        "                    nn.LayerNorm(emb_size),\n",
        "                    nn.Sequential(\n",
        "                        spectral_norm(nn.Linear(emb_size, forward_expansion * emb_size)),\n",
        "                        nn.GELU(),\n",
        "                        nn.Dropout(forward_drop_p),\n",
        "                        spectral_norm(nn.Linear(forward_expansion * emb_size, emb_size)),\n",
        "                    ),\n",
        "                    nn.Dropout(drop_p)\n",
        "                )\n",
        "            ))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6IYQuoNg9EA"
      },
      "source": [
        "class DiscriminatorTransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth=4, **kwargs):\n",
        "        super().__init__(*[DiscriminatorTransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
        "\n",
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size=384, class_size_1=4098, class_size_2=1024, class_size_3=512, n_classes=10):\n",
        "        super().__init__(\n",
        "            nn.LayerNorm(emb_size),\n",
        "            spectral_norm(nn.Linear(emb_size, class_size_1)),\n",
        "            nn.GELU(),\n",
        "            spectral_norm(nn.Linear(class_size_1, class_size_2)),\n",
        "            nn.GELU(),\n",
        "            spectral_norm(nn.Linear(class_size_2, class_size_3)),\n",
        "            nn.GELU(),\n",
        "            spectral_norm(nn.Linear(class_size_3, n_classes)),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Take only the cls token outputs\n",
        "        x = x[:, 0, :]\n",
        "        return super().forward(x)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oALpYkMhCGV"
      },
      "source": [
        "class ViT(nn.Sequential):\n",
        "    def __init__(self,     \n",
        "                in_channels=3,\n",
        "                patch_size=4,\n",
        "                stride_size=4,\n",
        "                emb_size=384,\n",
        "                image_size=32,\n",
        "                depth=4,\n",
        "                n_classes=1,\n",
        "                diffaugment='color,translation,cutout',\n",
        "                **kwargs):\n",
        "        self.diffaugment = diffaugment\n",
        "        super().__init__(\n",
        "            PatchEmbedding(in_channels, patch_size, stride_size, emb_size, image_size),\n",
        "            DiscriminatorTransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
        "            ClassificationHead(emb_size, n_classes=n_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, img, do_augment=True):\n",
        "        if do_augment:\n",
        "            img = DiffAugment(img, policy=self.diffaugment)\n",
        "        return super().forward(img)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu4HNFgpaLZ2"
      },
      "source": [
        "# CNN Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyj4W0kQLJ2j"
      },
      "source": [
        "class CNN(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                diffaugment='color,translation,cutout',\n",
        "                **kwargs):\n",
        "        self.diffaugment = diffaugment\n",
        "        super().__init__(\n",
        "            nn.Conv2d(3,32,kernel_size=3,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32,64,kernel_size=3,stride=1,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "\n",
        "            nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128,128,kernel_size=3,stride=1,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "\n",
        "            nn.Conv2d(128,256,kernel_size=3,stride=1,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256,256,kernel_size=3,stride=1,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256*4*4,1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024,512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, img, do_augment=True):\n",
        "        if do_augment:\n",
        "            img = DiffAugment(img, policy=self.diffaugment)\n",
        "        return super().forward(img)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbniFkThhR5T"
      },
      "source": [
        "# StyleGAN2 Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW_m-p4pg34m"
      },
      "source": [
        "class StyleGanDiscriminator(stylegan2_pytorch.Discriminator):\n",
        "    def __init__(self,\n",
        "                diffaugment='color,translation,cutout',\n",
        "                **kwargs):\n",
        "        self.diffaugment = diffaugment\n",
        "        super().__init__(**kwargs)\n",
        "    def forward(self, img, do_augment=True):\n",
        "        if do_augment:\n",
        "            img = DiffAugment(img, policy=self.diffaugment)\n",
        "        out, _ = super().forward(img)\n",
        "        return out"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Akp8Ybo483q"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS5sfmIuLj8P"
      },
      "source": [
        "# Create the Generator\n",
        "Generator = GeneratorViT(   patch_size=patch_size,\n",
        "                            image_size=image_size,\n",
        "                            latent_dim=latent_dim,\n",
        "                            combine_patch_embeddings=combine_patch_embeddings,\n",
        "                            combined_embedding_size=combined_embedding_size,\n",
        "                            sln_paremeter_size=sln_paremeter_size,\n",
        "                            num_heads = num_heads,\n",
        "                            depth = depth,\n",
        "                         ).to(device)\n",
        "\n",
        "# Create the SIREN network\n",
        "img_siren = Siren(in_features=siren_in_features, out_features=out_features, hidden_features=hidden_features,\n",
        "                  hidden_layers=3, outermost_linear=True).to(device)\n",
        "\n",
        "# Create the two types of discriminators\n",
        "Discriminator = ViT(discriminator=True,\n",
        "                            stride_size=patch_size*2,\n",
        "                            n_classes=1, \n",
        "                            num_heads = num_heads,\n",
        "                            depth = depth,\n",
        "                    ).to(device)\n",
        "cnn_discriminator = CNN().to(device)\n",
        "stylegan2_discriminator = StyleGanDiscriminator(image_size=32).to(device)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9EdXO0K8qFW"
      },
      "source": [
        "# Testing the generator\n",
        "\n",
        "Train to match fixed latent values to fixed images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BsgyaZMMi2f"
      },
      "source": [
        "total_steps = 1001 # Since the whole image is our dataset, this just means 500 gradient descent steps.\n",
        "steps_til_summary = 10\n",
        "\n",
        "lr = 3e-5\n",
        "\n",
        "params = list(Generator.parameters()) + list(img_siren.parameters())\n",
        "optim = torch.optim.Adam(lr=lr, params=params)\n",
        "ema = ExponentialMovingAverage(params, decay=0.995)\n",
        "\n",
        "ground_truth, _ = next(iter(trainloader))\n",
        "ground_truth = ground_truth.permute(0, 2, 3, 1).view((-1, image_size**2, out_features))\n",
        "ground_truth = ground_truth.to(device)\n",
        "\n",
        "z = torch.randn([batch_size, latent_dim]).to(device)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swv4ZQt94833"
      },
      "source": [
        "for step in range(total_steps):\n",
        "    hidden = Generator(z)\n",
        "    siren_input = mix_hidden_and_pos(hidden).to(device)\n",
        "    model_output, coords = img_siren(siren_input)\n",
        "    # convert model_output from [batch_size*num_patches, patch_size^2, out_features]\n",
        "    # to [batch_size, image_size^2, out_features]\n",
        "    model_output = model_output.view([-1, image_size**2, out_features])\n",
        "    loss = ((model_output - ground_truth)**2).mean()\n",
        "    \n",
        "    if not step % steps_til_summary:\n",
        "        print(\"Step %d, Total loss %0.6f\" % (step, loss))\n",
        "\n",
        "        fig, axes = plt.subplots(2,8, figsize=(24,6))\n",
        "        for i in range(8):\n",
        "            j = np.random.randint(0, batch_size-1)\n",
        "            img = model_output[j].cpu().view(32,32,3).detach().numpy()\n",
        "            img -= img.min()\n",
        "            img /= img.max()\n",
        "            axes[0,i].imshow(img)\n",
        "            g_img = ground_truth[j].cpu().view(32,32,3).detach().numpy()\n",
        "            g_img -= g_img.min()\n",
        "            g_img /= g_img.max()\n",
        "            axes[1,i].imshow(g_img)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    ema.update()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeyS_aTm9oII"
      },
      "source": [
        "%tensorboard --logdir runs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcWiPCochjJ7"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad_UXnQJCOeN"
      },
      "source": [
        "os.makedirs(\"weights\", exist_ok = True)\n",
        "os.makedirs(\"samples\", exist_ok = True)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "if discriminator_type == \"cnn\": discriminator = cnn_discriminator\n",
        "elif discriminator_type == \"stylegan2\": discriminator = stylegan2_discriminator\n",
        "elif discriminator_type == \"vitgan\": discriminator = Discriminator\n",
        "\n",
        "params = list(Generator.parameters()) + list(img_siren.parameters())\n",
        "optim_g = torch.optim.Adam(lr=lr, params=params, betas=beta)\n",
        "optim_d = torch.optim.Adam(lr=lr, params=discriminator.parameters(), betas=beta)\n",
        "ema = ExponentialMovingAverage(params, decay=0.995)\n",
        "\n",
        "fixed_noise = torch.FloatTensor(np.random.normal(0, 1, (16, latent_dim))).to(device)\n",
        "\n",
        "discriminator_f_img = torch.zeros([batch_size, 3, image_size, image_size]).to(device)\n",
        "\n",
        "trainset_len = len(trainloader.dataset)\n",
        "\n",
        "step = 0\n",
        "for epoch in range(epochs):\n",
        "    for batch_id, batch in enumerate(trainloader):\n",
        "        step += 1\n",
        "\n",
        "        # Train discriminator\n",
        "\n",
        "        # Forward + Backward with real images\n",
        "        r_img = batch[0].to(device)\n",
        "        r_logit = discriminator(r_img).flatten()\n",
        "        r_label = torch.ones(r_logit.shape[0]).to(device)\n",
        "\n",
        "        lossD_real = criterion(r_logit, r_label)\n",
        "        \n",
        "        lossD_bCR_real = F.mse_loss(r_logit, discriminator(r_img, do_augment=False))\n",
        "\n",
        "        # Forward + Backward with fake images\n",
        "        latent_vector = torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))).to(device)\n",
        "\n",
        "        hidden = Generator(latent_vector)\n",
        "        model_input = mix_hidden_and_pos(hidden).to(device)\n",
        "        model_output, coords = img_siren(model_input)\n",
        "        # convert model_output from [batch_size*num_patches, patch_size^2, out_features]\n",
        "        f_img = model_output.view([-1, image_size, image_size, out_features])\n",
        "        f_img = f_img.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        f_label = torch.zeros(batch_size).to(device)\n",
        "        # Save the a single generated image to the discriminator training data\n",
        "        if batch_size_history_discriminator:\n",
        "            discriminator_f_img[step % batch_size] = f_img[0].detach()\n",
        "            f_logit_history = discriminator(discriminator_f_img).flatten()\n",
        "            lossD_fake_history = criterion(f_logit_history, f_label)\n",
        "        else: lossD_fake_history = 0\n",
        "        # Train the discriminator on the images, generated only from this batch\n",
        "        f_logit = discriminator(f_img).flatten()\n",
        "        lossD_fake = criterion(f_logit, f_label)\n",
        "        \n",
        "        lossD_bCR_fake = F.mse_loss(f_logit, discriminator(f_img, do_augment=False))\n",
        "        \n",
        "        f_noise_input = torch.FloatTensor(np.random.rand(*f_img.shape)*2 - 1).to(device)\n",
        "        f_noise_logit = discriminator(f_noise_input).flatten()\n",
        "        lossD_noise = criterion(f_noise_logit, f_label)\n",
        "\n",
        "        lossD = lossD_real * 0.5 +\\\n",
        "                lossD_fake * 0.5 +\\\n",
        "                lossD_fake_history * lambda_lossD_history +\\\n",
        "                lossD_noise * lambda_lossD_noise +\\\n",
        "                lossD_bCR_real * lambda_bCR_real +\\\n",
        "                lossD_bCR_fake * lambda_bCR_fake\n",
        "\n",
        "        optim_d.zero_grad()\n",
        "        lossD.backward()\n",
        "        optim_d.step()\n",
        "        \n",
        "        # Train Generator\n",
        "\n",
        "        latent_vector = torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))).to(device)\n",
        "        hidden = Generator(latent_vector)\n",
        "        model_input = mix_hidden_and_pos(hidden).to(device)\n",
        "        model_output, coords = img_siren(model_input)\n",
        "        # convert model_output from [batch_size*num_patches, patch_size^2, out_features]\n",
        "        f_img = model_output.view([-1, image_size, image_size, out_features])\n",
        "        f_img = f_img.permute(0, 3, 1, 2)\n",
        "\n",
        "        f_logit = discriminator(f_img).flatten()\n",
        "        r_label = torch.ones(batch_size).to(device)\n",
        "        lossG = criterion(f_logit, r_label)\n",
        "        \n",
        "        optim_g.zero_grad()\n",
        "        lossG.backward()\n",
        "        optim_g.step()\n",
        "        ema.update()\n",
        "\n",
        "        writer.add_scalar(\"Loss/Generator\", lossG, step)\n",
        "        writer.add_scalar(\"Loss/Dis(real)\", lossD_real, step)\n",
        "        writer.add_scalar(\"Loss/Dis(fake)\", lossD_fake, step)\n",
        "        writer.add_scalar(\"Loss/Dis(fake_history)\", lossD_fake_history, step)\n",
        "        writer.add_scalar(\"Loss/Dis(noise)\", lossD_noise, step)\n",
        "        writer.add_scalar(\"Loss/Dis(bCR_fake)\", lossD_bCR_fake * lambda_bCR_fake, step)\n",
        "        writer.add_scalar(\"Loss/Dis(bCR_real)\", lossD_bCR_real * lambda_bCR_real, step)\n",
        "        writer.flush()\n",
        "\n",
        "        if batch_id%20 == 0:\n",
        "            print(f'epoch {epoch}/{epochs}; batch {batch_id}/{int(trainset_len/batch_size)}')\n",
        "            print(f'Generator: {\"{:.3f}\".format(float(lossG))}, '+\\\n",
        "                  f'Dis(real): {\"{:.3f}\".format(float(lossD_real))}, '+\\\n",
        "                  f'Dis(fake): {\"{:.3f}\".format(float(lossD_fake))}, '+\\\n",
        "                  f'Dis(fake_history): {\"{:.3f}\".format(float(lossD_fake_history))}, '+\\\n",
        "                  f'Dis(noise) {\"{:.3f}\".format(float(lossD_noise))}, '+\\\n",
        "                  f'Dis(bCR_fake): {\"{:.3f}\".format(float(lossD_bCR_fake * lambda_bCR_fake))}, '+\\\n",
        "                  f'Dis(bCR_real): {\"{:.3f}\".format(float(lossD_bCR_real * lambda_bCR_real))}')\n",
        "\n",
        "            # Plot 8 randomly selected samples\n",
        "            fig, axes = plt.subplots(1,8, figsize=(24,3))\n",
        "            for i in range(8):\n",
        "                j = np.random.randint(0, batch_size-1)\n",
        "                model_output = model_output.view([-1, image_size**2, out_features])\n",
        "                img = model_output[j].cpu().view(32,32,3).detach().numpy()\n",
        "                img -= img.min()\n",
        "                img /= img.max()\n",
        "                axes[i].imshow(img)\n",
        "            plt.show()\n",
        "\n",
        "    # if step % sample_interval == 0:\n",
        "    Generator.eval()\n",
        "    hidden = Generator(fixed_noise)\n",
        "    model_input = mix_hidden_and_pos(hidden).to(device)\n",
        "    model_output, coords = img_siren(model_input)\n",
        "    f_img = model_output.view([-1, image_size, image_size, out_features])\n",
        "    vis = f_img.permute(0, 3, 1, 2)\n",
        "    vis.detach().cpu()\n",
        "    vis = make_grid(vis, nrow = 4, padding = 5, normalize = True)\n",
        "    \n",
        "    writer.add_image(f'Generated/epoch_{epoch}', vis)\n",
        "\n",
        "    vis = T.ToPILImage()(vis)\n",
        "    vis.save('samples/vis{:03d}.jpg'.format(epoch))\n",
        "    Generator.train()\n",
        "    print(\"Save sample to samples/vis{:03d}.jpg\".format(epoch))\n",
        "\n",
        "    # Save the checkpoints.\n",
        "    torch.save(Generator.state_dict(), 'weights/Generator.pth')\n",
        "    torch.save(discriminator.state_dict(), 'weights/discriminator.pth')\n",
        "    print(\"Save model state.\")\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6FpZjDr-n5T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}