{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiVyqRSgDTpU"
      },
      "source": [
        "# ViTGAN pytorch implementation\n",
        "\n",
        "This notebook is a pytorch implementation of [VITGAN: Training GANs with Vision Transformers](https://arxiv.org/pdf/2107.04589v1.pdf)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1kJJw6BYW01HgooCZ2zUDt54e1mXqITXH?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjz0JmQyD4JJ"
      },
      "source": [
        "The model consists of a Vision Transformer Generator and a Vision Transformer Discriminator.\n",
        "\n",
        "It is adversarially trained to map latent vectors to images, which closely resemble the images from a given dataset. In this implementation, the dataset used is CIFAR-10.\n",
        "\n",
        "The Generator takes latent values $z$ as input, which is integrated in a Vision Transformer Encoder. The output for each patch of the image is fed to a SIREN network, in combination with a Fourier Embedding ($E_{fou}$)\n",
        "\n",
        "![ViTGAN Generator architecture](https://drive.google.com/uc?export=view&id=1XaCVOLq8Bvg-I3qM-bugNZcjIW5L7XTO)\n",
        "\n",
        "This implementation separates the Generator in Vision Transformer and SIREN networks for debugging purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZeCyoxmxwud"
      },
      "source": [
        "1.   [x] Use vectorized L2 distance in attention for **Discriminator**\n",
        "2.   [x] Overlapping Image Patches\n",
        "2.   [x] DiffAugment\n",
        "3.   [x] Self-modulated LayerNorm (SLN)\n",
        "4.   [x] Implicit Neural Representation for Patch Generation\n",
        "5.   [x] ExponentialMovingAverage (EMA)\n",
        "6.   [x] Balanced Consistency Regularization (bCR)\n",
        "7.   [x] Improved Spectral Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_gqyo3DZA3J"
      },
      "outputs": [],
      "source": [
        "! pip install einops\n",
        "! pip install git+https://github.com/fadel/pytorch_ema\n",
        "! pip install stylegan2-pytorch\n",
        "! pip install tensorboard\n",
        "! pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TufykVSpr-QU"
      },
      "outputs": [],
      "source": [
        "! wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXCe_q68481Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from torch_ema import ExponentialMovingAverage\n",
        "\n",
        "from stylegan2_pytorch import stylegan2_pytorch\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-JOToTeB9uE"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeQq8IuCAPmZ"
      },
      "outputs": [],
      "source": [
        "image_size = 32\n",
        "style_mlp_layers = 8\n",
        "patch_size = 4\n",
        "latent_dim = 512 # Size of z\n",
        "hidden_size = 384\n",
        "depth = 4\n",
        "num_heads = 4\n",
        "\n",
        "dropout_p = 0.\n",
        "bias = True\n",
        "weight_modulation = True\n",
        "demodulation = False\n",
        "siren_hidden_layers = 1\n",
        "\n",
        "combine_patch_embeddings = False # Generate an image from a single SIREN, instead of patch-by-patch\n",
        "combine_patch_embeddings_size = hidden_size * 4\n",
        "\n",
        "sln_paremeter_size = hidden_size # either hidden_size or 1\n",
        "\n",
        "batch_size = 50\n",
        "device = \"cuda\"\n",
        "out_features = 3 # The number of color channels\n",
        "\n",
        "generator_type = \"vitgan\" # \"vitgan\", \"cnn\"\n",
        "discriminator_type = \"vitgan\" # \"vitgan\", \"cnn\", \"stylegan2\"\n",
        "\n",
        "lr = 7e-4 # Learning rate\n",
        "lr_dis = 7e-4 # Learning rate\n",
        "beta = (0., 0.99) # Adam oprimizer parameters for both the generator and the discriminator\n",
        "batch_size_history_discriminator = False # Whether to use a loss, which tracks one sample from last batch_size number of batches\n",
        "epochs = 400 # Number of epochs\n",
        "lambda_bCR_real = 10\n",
        "lambda_bCR_fake = 10\n",
        "lambda_lossD_noise = 0.0\n",
        "lambda_lossD_history = 0.0\n",
        "lambda_diversity_penalty = 0.0\n",
        "\n",
        "experiment_folder_name = f'lr-{lr}_\\\n",
        "lr_dis-{lr_dis}_\\\n",
        "bias-{bias}_\\\n",
        "demod-{demodulation}_\\\n",
        "sir_n_layer-{siren_hidden_layers}_\\\n",
        "w_mod-{weight_modulation}_\\\n",
        "patch_s-{patch_size}_\\\n",
        "st_mlp_l-{style_mlp_layers}_\\\n",
        "hid_size-{hidden_size}_\\\n",
        "comb_patch_emb-{combine_patch_embeddings}_\\\n",
        "sln_par_s-{sln_paremeter_size}_\\\n",
        "dis_type-{discriminator_type}_\\\n",
        "gen_type-{generator_type}_\\\n",
        "n_head-{num_heads}_\\\n",
        "depth-{depth}_\\\n",
        "drop_p-{dropout_p}_\\\n",
        "l_bCR_r-{lambda_bCR_real}_\\\n",
        "l_bCR_f-{lambda_bCR_fake}_\\\n",
        "l_D_noise-{lambda_lossD_noise}_\\\n",
        "l_D_his-{lambda_lossD_history}\\\n",
        "'\n",
        "writer = SummaryWriter(log_dir=experiment_folder_name)\n",
        "\n",
        "wandb.init(project='ViTGAN-pytorch')\n",
        "config = wandb.config\n",
        "config.image_size = image_size\n",
        "config.bias = bias\n",
        "config.demodulation = demodulation\n",
        "config.siren_hidden_layers = siren_hidden_layers\n",
        "config.weight_modulation = weight_modulation\n",
        "config.style_mlp_layers = style_mlp_layers\n",
        "config.patch_size = patch_size\n",
        "config.latent_dim = latent_dim\n",
        "config.hidden_size = hidden_size\n",
        "config.depth = depth\n",
        "config.num_heads = num_heads\n",
        "\n",
        "config.dropout_p = dropout_p\n",
        "\n",
        "config.combine_patch_embeddings = combine_patch_embeddings\n",
        "config.combine_patch_embeddings_size = combine_patch_embeddings_size\n",
        "\n",
        "config.sln_paremeter_size = sln_paremeter_size\n",
        "\n",
        "config.batch_size = batch_size\n",
        "config.device = device\n",
        "config.out_features = out_features\n",
        "\n",
        "config.generator_type = generator_type\n",
        "config.discriminator_type = discriminator_type\n",
        "\n",
        "config.lr = lr\n",
        "config.lr_dis = lr_dis\n",
        "config.beta1 = beta[0]\n",
        "config.beta2 = beta[1]\n",
        "config.batch_size_history_discriminator = batch_size_history_discriminator\n",
        "config.epochs = epochs\n",
        "config.lambda_bCR_real = lambda_bCR_real\n",
        "config.lambda_bCR_fake = lambda_bCR_fake\n",
        "config.lambda_lossD_noise = lambda_lossD_noise\n",
        "config.lambda_lossD_history = lambda_lossD_history\n",
        "config.lambda_diversity_penalty = lambda_diversity_penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3dGermsa1bG"
      },
      "outputs": [],
      "source": [
        "if combine_patch_embeddings:\n",
        "    out_patch_size = image_size\n",
        "    combined_embedding_size = combine_patch_embeddings_size\n",
        "else:\n",
        "    out_patch_size = patch_size\n",
        "    combined_embedding_size = hidden_size\n",
        "\n",
        "siren_in_features = combined_embedding_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M4De21AB8lE"
      },
      "source": [
        "\n",
        "\n",
        "https://github.com/mit-han-lab/data-efficient-gans/blob/master/DiffAugment-stylegan2-pytorch/DiffAugment_pytorch.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsaytJSj0inJ"
      },
      "outputs": [],
      "source": [
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.1):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2).contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.3):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA-uwpp7b1QK"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0., 0., 0.), (1., 1., 1.))\n",
        "    ])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63v16amHIG2L"
      },
      "source": [
        "Visualize the effects of the DiffAugment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnQW4nBraOJb"
      },
      "outputs": [],
      "source": [
        "img = next(iter(trainloader))[0]\n",
        "img = DiffAugment(img, policy='color,translation,cutout', channels_first=True)\n",
        "img = img.permute(0,2,3,1)[0]\n",
        "img -= img.min()\n",
        "img /= img.max()\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QKaT8pk9vD_"
      },
      "source": [
        "# Improved Spectral Normalization (ISN)\n",
        "\n",
        "$$\n",
        "\\bar{W}_{ISN}(W):=\\sigma(W_{init})\\cdot W/\\sigma(W)\n",
        "$$\n",
        "\n",
        "Reference code: https://github.com/koshian2/SNGAN\n",
        "\n",
        "When updating the weights, normalize the weights' norm to its norm at initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BL2ZLEnM-xct"
      },
      "outputs": [],
      "source": [
        "def l2normalize(v, eps=1e-4):\n",
        "\treturn v / (v.norm() + eps)\n",
        "\n",
        "class spectral_norm(nn.Module):\n",
        "\tdef __init__(self, module, name='weight', power_iterations=1):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.module = module\n",
        "\t\tself.name = name\n",
        "\t\tself.power_iterations = power_iterations\n",
        "\t\tif not self._made_params():\n",
        "\t\t\tself._make_params()\n",
        "\t\tself.w_init_sigma = None\n",
        "\t\tself.w_initalized = False\n",
        "\n",
        "\tdef _update_u_v(self):\n",
        "\t\tu = getattr(self.module, self.name + \"_u\")\n",
        "\t\tv = getattr(self.module, self.name + \"_v\")\n",
        "\t\tw = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "\t\theight = w.data.shape[0]\n",
        "\t\t_w = w.view(height, -1)\n",
        "\t\tfor _ in range(self.power_iterations):\n",
        "\t\t\tv = l2normalize(torch.matmul(_w.t(), u))\n",
        "\t\t\tu = l2normalize(torch.matmul(_w, v))\n",
        "\n",
        "\t\tsigma = u.dot((_w).mv(v))\n",
        "\t\tif not self.w_initalized:\n",
        "\t\t\tself.w_init_sigma = np.array(sigma.expand_as(w).detach().cpu())\n",
        "\t\t\tself.w_initalized = True\n",
        "\t\tsetattr(self.module, self.name, torch.tensor(self.w_init_sigma).to(device) * w / sigma.expand_as(w))\n",
        "\n",
        "\tdef _made_params(self):\n",
        "\t\ttry:\n",
        "\t\t\tgetattr(self.module, self.name + \"_u\")\n",
        "\t\t\tgetattr(self.module, self.name + \"_v\")\n",
        "\t\t\tgetattr(self.module, self.name + \"_bar\")\n",
        "\t\t\treturn True\n",
        "\t\texcept AttributeError:\n",
        "\t\t\treturn False\n",
        "\n",
        "\tdef _make_params(self):\n",
        "\t\tw = getattr(self.module, self.name)\n",
        "\n",
        "\t\theight = w.data.shape[0]\n",
        "\t\twidth = w.view(height, -1).data.shape[1]\n",
        "\n",
        "\t\tu = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "\t\tv = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "\t\tu.data = l2normalize(u.data)\n",
        "\t\tv.data = l2normalize(v.data)\n",
        "\t\tw_bar = Parameter(w.data)\n",
        "\n",
        "\t\tdel self.module._parameters[self.name]\n",
        "\t\tself.module.register_parameter(self.name + \"_u\", u)\n",
        "\t\tself.module.register_parameter(self.name + \"_v\", v)\n",
        "\t\tself.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "\tdef forward(self, *args):\n",
        "\t\tself._update_u_v()\n",
        "\t\treturn self.module.forward(*args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttk-7hLuIZ4o"
      },
      "source": [
        "Vision Transformer reference code: \\[[Blog Post](https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632)\\]\n",
        "\n",
        "Normal Attention Mechanism\n",
        "\n",
        "$$\n",
        "Attention_h(X) = softmax \\bigg ( \\frac{QK^T}{\\sqrt{d_h}} V \\bigg )\n",
        "$$\n",
        "\n",
        "Lipschitz Attention Mechanism\n",
        "\n",
        "$$\n",
        "Attention_h(X) = softmax \\bigg ( \\frac{d(Q,K)}{\\sqrt{d_h}} V \\bigg )\n",
        "$$\n",
        "\n",
        "where $d(Q,K)$ is L2-distance.\n",
        "\n",
        "https://arxiv.org/pdf/2006.04710.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1VgcIu6MFow"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size=384, num_heads=4, dropout=0, discriminator=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.discriminator = discriminator\n",
        "        # fuse the queries, keys and values in one matrix\n",
        "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "        if self.discriminator:\n",
        "            self.qkv = spectral_norm(self.qkv)\n",
        "            self.projection = spectral_norm(self.projection)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        # split keys, queries and values in num_heads\n",
        "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
        "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
        "        if self.discriminator:\n",
        "            # calculate L2-distances\n",
        "            energy = torch.cdist(queries.contiguous(), keys.contiguous(), p=2)\n",
        "        else:\n",
        "            # sum up over the last axis\n",
        "            energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
        "\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "            \n",
        "        scaling = self.emb_size ** (1/2)\n",
        "        att = F.softmax(energy, dim=-1) / scaling\n",
        "        att = self.att_drop(att)\n",
        "        # sum up over the third axis\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpUuzMsS9G-t"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xwQ0P_83N6L"
      },
      "outputs": [],
      "source": [
        "class FullyConnectedLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "        in_features,                # Number of input features.\n",
        "        out_features,               # Number of output features.\n",
        "        bias            = True,     # Apply additive bias before the activation function?\n",
        "        activation      = 'linear', # Activation function: 'relu', 'lrelu', etc.\n",
        "        lr_multiplier   = 1,        # Learning rate multiplier.\n",
        "        bias_init       = 0,        # Initial value for the additive bias.\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        if activation == 'lrelu':\n",
        "            self.activation = nn.LeakyReLU(0.2)\n",
        "        elif activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'gelu':\n",
        "            self.activation = nn.GELU()\n",
        "        self.weight = nn.Parameter(torch.randn([out_features, in_features]) / lr_multiplier)\n",
        "        self.bias = nn.Parameter(torch.full([out_features], np.float32(bias_init))) if bias else None\n",
        "        self.weight_gain = lr_multiplier / np.sqrt(in_features)\n",
        "        self.bias_gain = lr_multiplier\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = self.weight.to(x.dtype) * self.weight_gain\n",
        "        b = self.bias\n",
        "        if b is not None:\n",
        "            b = b.to(x.dtype)\n",
        "            if self.bias_gain != 1:\n",
        "                b = b * self.bias_gain\n",
        "\n",
        "        if self.activation == 'linear' and b is not None:\n",
        "            # print(b.shape, x.shape, w.t().shape)\n",
        "            x = torch.addmm(b.unsqueeze(0), x, w.t())\n",
        "        else:\n",
        "            x = x.matmul(w.t())\n",
        "            if b is not None:\n",
        "                x = x + b\n",
        "            if self.activation != 'linear':\n",
        "                x = self.activation(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx-xGRh_61Qz"
      },
      "outputs": [],
      "source": [
        "def normalize_2nd_moment(x, dim=1, eps=1e-8):\n",
        "    return x * (x.square().mean(dim=dim, keepdim=True) + eps).rsqrt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCHXNmaF5C6m"
      },
      "outputs": [],
      "source": [
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "        z_dim,                      # Input latent (Z) dimensionality, 0 = no latent.\n",
        "        c_dim,                      # Conditioning label (C) dimensionality, 0 = no label.\n",
        "        w_dim,                      # Intermediate latent (W) dimensionality.\n",
        "        num_ws          = None,     # Number of intermediate latents to output, None = do not broadcast.\n",
        "        num_layers      = 8,        # Number of mapping layers.\n",
        "        embed_features  = None,     # Label embedding dimensionality, None = same as w_dim.\n",
        "        layer_features  = None,     # Number of intermediate features in the mapping layers, None = same as w_dim.\n",
        "        activation      = 'lrelu',  # Activation function: 'relu', 'lrelu', etc.\n",
        "        lr_multiplier   = 0.01,     # Learning rate multiplier for the mapping layers.\n",
        "        w_avg_beta      = 0.995,    # Decay for tracking the moving average of W during training, None = do not track.\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.c_dim = c_dim\n",
        "        self.w_dim = w_dim\n",
        "        self.num_ws = num_ws\n",
        "        self.num_layers = num_layers\n",
        "        self.w_avg_beta = w_avg_beta\n",
        "\n",
        "        if embed_features is None:\n",
        "            embed_features = w_dim\n",
        "        if c_dim == 0:\n",
        "            embed_features = 0\n",
        "        if layer_features is None:\n",
        "            layer_features = w_dim\n",
        "        features_list = [z_dim + embed_features] + [layer_features] * (num_layers - 1) + [w_dim]\n",
        "\n",
        "        if c_dim > 0:\n",
        "            self.embed = FullyConnectedLayer(c_dim, embed_features)\n",
        "        for idx in range(num_layers):\n",
        "            in_features = features_list[idx]\n",
        "            out_features = features_list[idx + 1]\n",
        "            layer = FullyConnectedLayer(in_features, out_features, activation=activation, lr_multiplier=lr_multiplier)\n",
        "            setattr(self, f'fc{idx}', layer)\n",
        "\n",
        "        if num_ws is not None and w_avg_beta is not None:\n",
        "            self.register_buffer('w_avg', torch.zeros([w_dim]))\n",
        "\n",
        "    def forward(self, z, c=None, truncation_psi=1, truncation_cutoff=None, skip_w_avg_update=False):\n",
        "        # Embed, normalize, and concat inputs.\n",
        "        x = None\n",
        "        with torch.autograd.profiler.record_function('input'):\n",
        "            if self.z_dim > 0:\n",
        "                assert z.shape[1] == self.z_dim\n",
        "                x = normalize_2nd_moment(z.to(torch.float32))\n",
        "            if self.c_dim > 0:\n",
        "                assert c.shape[1] == self.c_dim\n",
        "                y = normalize_2nd_moment(self.embed(c.to(torch.float32)))\n",
        "                x = torch.cat([x, y], dim=1) if x is not None else y\n",
        "\n",
        "        # Main layers.\n",
        "        for idx in range(self.num_layers):\n",
        "            layer = getattr(self, f'fc{idx}')\n",
        "            x = layer(x)\n",
        "\n",
        "        # Update moving average of W.\n",
        "        if self.w_avg_beta is not None and self.training and not skip_w_avg_update:\n",
        "            with torch.autograd.profiler.record_function('update_w_avg'):\n",
        "                self.w_avg.copy_(x.detach().mean(dim=0).lerp(self.w_avg, self.w_avg_beta))\n",
        "\n",
        "        # Broadcast.\n",
        "        if self.num_ws is not None:\n",
        "            with torch.autograd.profiler.record_function('broadcast'):\n",
        "                x = x.unsqueeze(1).repeat([1, self.num_ws, 1])\n",
        "\n",
        "        # Apply truncation.\n",
        "        if truncation_psi != 1:\n",
        "            with torch.autograd.profiler.record_function('truncate'):\n",
        "                assert self.w_avg_beta is not None\n",
        "                if self.num_ws is None or truncation_cutoff is None:\n",
        "                    x = self.w_avg.lerp(x, truncation_psi)\n",
        "                else:\n",
        "                    x[:, :truncation_cutoff] = self.w_avg.lerp(x[:, :truncation_cutoff], truncation_psi)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GJiP09nL_Lh"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, expansion=4, drop_p=0., bias=False):\n",
        "        super().__init__(\n",
        "            FullyConnectedLayer(expansion, emb_size * emb_size, activation='gelu', bias=False),\n",
        "            nn.Dropout(drop_p),\n",
        "            FullyConnectedLayer(expansion * emb_size, emb_size, bias=False),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4713MviFIl8R"
      },
      "source": [
        "Self-Modulated LayerNorm\n",
        "$$\n",
        "SLN(h_{\\ell},w)=\\gamma_{\\ell}(w)\\odot\\frac{h_{\\ell}-\\mu}{\\sigma}+\\beta_{\\ell}(w)\n",
        "$$\n",
        "\n",
        "where $\\gamma_{\\ell}, \\beta_{\\ell}\\in \\mathbb{R}^D$ or $\\gamma_{\\ell}, \\beta_{\\ell}\\in \\mathbb{R}^1$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdd9c_sk9GTC"
      },
      "outputs": [],
      "source": [
        "class SLN(nn.Module):\n",
        "    def __init__(self, input_size, parameter_size=None, **kwargs):\n",
        "        super().__init__()\n",
        "        if parameter_size == None:\n",
        "            parameter_size = input_size\n",
        "        assert(input_size == parameter_size or parameter_size == 1)\n",
        "        self.input_size = input_size\n",
        "        self.parameter_size = parameter_size\n",
        "        self.ln = nn.LayerNorm(input_size)\n",
        "        self.gamma = FullyConnectedLayer(input_size, parameter_size, bias=False)\n",
        "        self.beta = FullyConnectedLayer(input_size, parameter_size, bias=False)\n",
        "        # self.gamma = nn.Linear(input_size, parameter_size, bias=False)\n",
        "        # self.beta = nn.Linear(input_size, parameter_size, bias=False)\n",
        "\n",
        "    def forward(self, hidden, w):\n",
        "        assert(hidden.size(-1) == self.parameter_size and w.size(-1) == self.parameter_size)\n",
        "        gamma = self.gamma(w).unsqueeze(1)\n",
        "        beta = self.beta(w).unsqueeze(1)\n",
        "        ln = self.ln(hidden)\n",
        "        return gamma * ln + beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvIYYkHe9J3f"
      },
      "outputs": [],
      "source": [
        "class GeneratorTransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hidden_size=384,\n",
        "                 sln_paremeter_size=384,\n",
        "                 drop_p=0.,\n",
        "                 forward_expansion=4,\n",
        "                 forward_drop_p=0.,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "        self.sln = SLN(hidden_size, parameter_size=sln_paremeter_size)\n",
        "        self.msa = MultiHeadAttention(hidden_size, **kwargs)\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "        self.feed_forward = FeedForwardBlock(hidden_size, expansion=forward_expansion, drop_p=forward_drop_p)\n",
        "\n",
        "    def forward(self, hidden, w):\n",
        "        res = hidden\n",
        "        hidden = self.sln(hidden, w)\n",
        "        hidden = self.msa(hidden)\n",
        "        hidden = self.dropout(hidden)\n",
        "        hidden += res\n",
        "\n",
        "        res = hidden\n",
        "        hidden = self.sln(hidden, w)\n",
        "        self.feed_forward(hidden)\n",
        "        hidden = self.dropout(hidden)\n",
        "        hidden += res\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXueAJqx9LsE"
      },
      "outputs": [],
      "source": [
        "class GeneratorTransformerEncoder(nn.Module):\n",
        "    def __init__(self, depth=4, **kwargs):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.blocks = nn.ModuleList([GeneratorTransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
        "    \n",
        "    def forward(self, hidden, w):\n",
        "        for i in range(self.depth):\n",
        "            hidden = self.blocks[i](hidden, w)\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiQtj0Qa9b7L"
      },
      "source": [
        "# SIREN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH09oAfX481V"
      },
      "source": [
        "Code for SIREN is taken from [SIREN reference colab notebook](https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOD98u3DzTow"
      },
      "source": [
        "$$\n",
        "w^{'}_{ijk}=s_i\\cdot w_{ijk}\n",
        "$$\n",
        "\n",
        "$$\n",
        "w^{''}_{ijk}=\\frac{w^{'}_{ijk}}{\\sqrt{\\sum_{i,k}{w^{'}_{ijk}}^2+\\epsilon}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm7OIWD5Y7Up"
      },
      "outputs": [],
      "source": [
        "class ModulatedLinear(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, style_size, bias=False, demodulation=True, **kwargs):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.style_size = style_size\n",
        "        self.scale = 1 / np.sqrt(in_channels)\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.randn(1, out_channels, in_channels, 1)\n",
        "        )\n",
        "        self.modulation = None\n",
        "        if self.style_size != self.in_channels:\n",
        "            self.modulation = FullyConnectedLayer(style_size, in_channels, bias=False)\n",
        "        self.demodulation = demodulation\n",
        "\n",
        "    def forward(self, input, style):\n",
        "        batch_size = input.shape[0]\n",
        "\n",
        "        if self.style_size != self.in_channels:\n",
        "            style = self.modulation(style)\n",
        "        style = style.view(batch_size, 1, self.in_channels, 1)\n",
        "        # print('self.scale, self.weight.shape, style.shape', self.scale, self.weight.shape, style.shape)\n",
        "        weight = self.scale * self.weight * style\n",
        "\n",
        "        if self.demodulation:\n",
        "            demod = torch.rsqrt(weight.pow(2).sum([2]) + 1e-8)\n",
        "            weight = weight * demod.view(batch_size, self.out_channels, 1, 1)\n",
        "\n",
        "        weight = weight.view(\n",
        "            batch_size * self.out_channels, self.in_channels, 1\n",
        "        )\n",
        "        \n",
        "        img_size = input.size(1)\n",
        "        input = input.reshape(1, batch_size * self.in_channels, img_size)\n",
        "        out = F.conv1d(input, weight, groups=batch_size)\n",
        "        out = out.view(batch_size, img_size, self.out_channels)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJv0hR38RBFC"
      },
      "outputs": [],
      "source": [
        "class ResLinear(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, style_size, bias=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.linear = FullyConnectedLayer(in_channels, out_channels, bias=False)\n",
        "        self.style = FullyConnectedLayer(style_size, in_channels, bias=False)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.style_size = style_size\n",
        "        # print('style_size, in_channels, out_channels', style_size, in_channels, out_channels)\n",
        "\n",
        "    def forward(self, input, style):\n",
        "        x = input + self.style(style).unsqueeze(1)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "930yhZ6zgPI0"
      },
      "outputs": [],
      "source": [
        "class ConLinear(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out, is_first=False, bias=True, **kwargs):\n",
        "        super(ConLinear, self).__init__()\n",
        "        self.conv = nn.Linear(ch_in, ch_out, bias=bias)\n",
        "        if is_first:\n",
        "            nn.init.uniform_(self.conv.weight, -np.sqrt(9 / ch_in), np.sqrt(9 / ch_in))\n",
        "        else:\n",
        "            nn.init.uniform_(self.conv.weight, -np.sqrt(3 / ch_in), np.sqrt(3 / ch_in))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class SinActivation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SinActivation, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sin(x)\n",
        "\n",
        "class LFF(nn.Module):\n",
        "    def __init__(self, hidden_size, **kwargs):\n",
        "        super(LFF, self).__init__()\n",
        "        self.ffm = ConLinear(2, hidden_size, is_first=True)\n",
        "        self.activation = SinActivation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x\n",
        "        x = self.ffm(x)\n",
        "        x = self.activation(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgjY_FpH481X"
      },
      "outputs": [],
      "source": [
        "class SineLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, style_size, bias=False,\n",
        "                 is_first=False, omega_0=30, weight_modulation=True, **kwargs):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "        \n",
        "        self.in_features = in_features\n",
        "        self.weight_modulation = weight_modulation\n",
        "        if weight_modulation:\n",
        "            self.linear = ModulatedLinear(in_features, out_features, style_size=style_size, bias=bias, **kwargs)\n",
        "        else:\n",
        "            self.linear = ResLinear(in_features, out_features, style_size=style_size, bias=bias, **kwargs)\n",
        "        # print('in_features, out_features, style_size', in_features, out_features, style_size)\n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                if self.weight_modulation:\n",
        "                    self.linear.weight.uniform_(-1 / self.in_features, \n",
        "                                                1 / self.in_features)\n",
        "                else:\n",
        "                    self.linear.linear.weight.uniform_(-1 / self.in_features, \n",
        "                                                        1 / self.in_features) \n",
        "            else:\n",
        "                if self.weight_modulation:\n",
        "                    self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n",
        "                                                np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "                else:\n",
        "                    self.linear.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n",
        "                                                        np.sqrt(6 / self.in_features) / self.omega_0) \n",
        "        \n",
        "    def forward(self, input, style):\n",
        "        return torch.sin(self.omega_0 * self.linear(input, style))\n",
        "    \n",
        "class Siren(nn.Module):\n",
        "    def __init__(self, in_features, hidden_size, hidden_layers, out_features, style_size, outermost_linear=False, \n",
        "                 first_omega_0=30, hidden_omega_0=30., weight_modulation=True, bias=False, **kwargs):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.net = []\n",
        "        self.net.append(SineLayer(in_features, hidden_size, style_size,\n",
        "                                  is_first=True, omega_0=first_omega_0,\n",
        "                                  weight_modulation=weight_modulation, **kwargs))\n",
        "\n",
        "        for i in range(hidden_layers):\n",
        "            self.net.append(SineLayer(hidden_size, hidden_size, style_size,\n",
        "                                      is_first=False, omega_0=hidden_omega_0,\n",
        "                                      weight_modulation=weight_modulation, **kwargs))\n",
        "\n",
        "        if outermost_linear:\n",
        "            if weight_modulation:\n",
        "                final_linear = ModulatedLinear(hidden_size, out_features,\n",
        "                                               style_size=style_size, bias=bias, **kwargs)\n",
        "            else:\n",
        "                final_linear = ResLinear(hidden_size, out_features, style_size=style_size, bias=bias, **kwargs)\n",
        "            # FullyConnectedLayer(hidden_size, out_features, bias=False)\n",
        "            # final_linear = nn.Linear(hidden_size, out_features)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                if weight_modulation:\n",
        "                    final_linear.weight.uniform_(-np.sqrt(6 / hidden_size) / hidden_omega_0, \n",
        "                                                np.sqrt(6 / hidden_size) / hidden_omega_0)\n",
        "                else:\n",
        "                    final_linear.linear.weight.uniform_(-np.sqrt(6 / hidden_size) / hidden_omega_0, \n",
        "                                                np.sqrt(6 / hidden_size) / hidden_omega_0)\n",
        "                \n",
        "            self.net.append(final_linear)\n",
        "        else:\n",
        "            self.net.append(SineLayer(hidden_size, out_features, \n",
        "                                      is_first=False, omega_0=hidden_omega_0,\n",
        "                                      weight_modulation=weight_modulation, **kwargs))\n",
        "        \n",
        "        self.net = nn.Sequential(*self.net)\n",
        "    \n",
        "    def forward(self, coords, style):\n",
        "        coords = coords.clone().detach().requires_grad_(True) # allows to take derivative w.r.t. input\n",
        "        # output = self.net(coords, style)\n",
        "        output = coords\n",
        "        for layer in self.net:\n",
        "            output = layer(output, style)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8DB73-TBGEs"
      },
      "source": [
        "$$\n",
        "Fou(\\mathbf{v})= \\left[ \\cos(2 \\pi \\mathbf B \\mathbf{v}), \\sin(2 \\pi \\mathbf B \\mathbf{v}) \\right]^\\mathrm{T}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ru0trNs9N3F"
      },
      "outputs": [],
      "source": [
        "class GeneratorViT(nn.Module):\n",
        "    def __init__(self,\n",
        "                style_mlp_layers=8,\n",
        "                patch_size=4,\n",
        "                latent_dim=32,\n",
        "                hidden_size=384,\n",
        "                sln_paremeter_size=1,\n",
        "                image_size=32,\n",
        "                depth=4,\n",
        "                combine_patch_embeddings=False,\n",
        "                combined_embedding_size=1024,\n",
        "                forward_drop_p=0.,\n",
        "                bias=False,\n",
        "                out_features=3,\n",
        "                weight_modulation=True,\n",
        "                siren_hidden_layers=1,\n",
        "                **kwargs):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.mlp = MappingNetwork(z_dim=latent_dim, c_dim=0, w_dim=hidden_size, num_layers=style_mlp_layers, w_avg_beta=None)\n",
        "\n",
        "        num_patches = int(image_size//patch_size)**2\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "        self.image_size = image_size\n",
        "        self.combine_patch_embeddings = combine_patch_embeddings\n",
        "        self.combined_embedding_size = combined_embedding_size\n",
        "\n",
        "        self.pos_emb = nn.Parameter(torch.randn(num_patches, hidden_size))\n",
        "        self.transformer_encoder = GeneratorTransformerEncoder(depth,\n",
        "                                                               hidden_size=hidden_size,\n",
        "                                                               sln_paremeter_size=sln_paremeter_size,\n",
        "                                                               drop_p=forward_drop_p,\n",
        "                                                               forward_drop_p=forward_drop_p,\n",
        "                                                               **kwargs)\n",
        "        self.sln = SLN(hidden_size, parameter_size=sln_paremeter_size)\n",
        "        if combine_patch_embeddings:\n",
        "            self.to_single_emb = nn.Sequential(\n",
        "                FullyConnectedLayer(num_patches*hidden_size, combined_embedding_size, bias=bias, activation='gelu'),\n",
        "                nn.Dropout(forward_drop_p),\n",
        "            )\n",
        "\n",
        "        self.lff = LFF(self.hidden_size)\n",
        "\n",
        "        self.siren_in_features = combined_embedding_size if combine_patch_embeddings else self.hidden_size\n",
        "        self.siren = Siren(in_features=self.siren_in_features, out_features=out_features,\n",
        "                           style_size=self.siren_in_features, hidden_size=self.hidden_size, bias=bias,\n",
        "                           hidden_layers=siren_hidden_layers, outermost_linear=True, weight_modulation=weight_modulation, **kwargs)\n",
        "\n",
        "        self.num_patches_x = int(image_size//out_patch_size)\n",
        "\n",
        "\n",
        "    def fourier_input_mapping(self, x):\n",
        "        return self.lff(x)\n",
        "\n",
        "    def fourier_pos_embedding(self):\n",
        "        # Create input pixel coordinates in the unit square\n",
        "        coords = np.linspace(-1, 1, out_patch_size, endpoint=True)\n",
        "        pos = np.stack(np.meshgrid(coords, coords), -1)\n",
        "        pos = torch.tensor(pos, dtype=torch.float).to(device)\n",
        "        result = self.fourier_input_mapping(pos).reshape([out_patch_size**2, self.hidden_size])\n",
        "        return result.to(device)\n",
        "\n",
        "    def mix_hidden_and_pos(self, hidden):\n",
        "        pos = self.fourier_pos_embedding()\n",
        "\n",
        "        pos = repeat(pos, 'p h -> n p h', n = hidden.shape[0])\n",
        "        result = pos\n",
        "\n",
        "        return result\n",
        "\n",
        "    def forward(self, z):\n",
        "        w = self.mlp(z)\n",
        "        pos = repeat(torch.sin(self.pos_emb), 'n e -> b n e', b=z.shape[0])\n",
        "        hidden = self.transformer_encoder(pos, w)\n",
        "\n",
        "        if self.combine_patch_embeddings:\n",
        "            # Output [batch_size, combined_embedding_size]\n",
        "            hidden = self.sln(hidden, w).view((z.shape[0], -1))\n",
        "            hidden = self.to_single_emb(hidden)\n",
        "        else:\n",
        "            # Output [batch_size*num_patches, hidden_size]\n",
        "            hidden = self.sln(hidden, w).view((-1, self.hidden_size))\n",
        "        \n",
        "        pos = self.mix_hidden_and_pos(hidden)\n",
        "\n",
        "        # hidden = repeat(hidden, 'n h -> n p h', p = out_patch_size**2)\n",
        "\n",
        "        result = self.siren(pos, hidden)\n",
        "\n",
        "        model_output_1 = result.view([-1, self.num_patches_x, self.num_patches_x, out_patch_size, out_patch_size, out_features])\n",
        "        model_output_2 = model_output_1.permute([0, 1, 3, 2, 4, 5])\n",
        "        model_output = model_output_2.reshape([-1, image_size**2, out_features])\n",
        "        \n",
        "        return model_output\n",
        "\n",
        "\n",
        "Generator = GeneratorViT(   patch_size=patch_size,\n",
        "                            image_size=image_size,\n",
        "                            style_mlp_layers=style_mlp_layers,\n",
        "                            latent_dim=latent_dim,\n",
        "                            hidden_size=hidden_size,\n",
        "                            combine_patch_embeddings=combine_patch_embeddings,\n",
        "                            combined_embedding_size=combined_embedding_size,\n",
        "                            sln_paremeter_size=sln_paremeter_size,\n",
        "                            num_heads=num_heads,\n",
        "                            depth=depth,\n",
        "                            forward_drop_p=dropout_p,\n",
        "                            bias=bias,\n",
        "                            weight_modulation=weight_modulation,\n",
        "                            siren_hidden_layers=siren_hidden_layers,\n",
        "                            demodulation=demodulation,\n",
        "                        ).to(device)\n",
        "print(Generator(torch.randn([batch_size, latent_dim]).to(device)).shape)\n",
        "# print(Generator)\n",
        "del Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqMxvqCjb2fp"
      },
      "source": [
        "# CNN Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7e7XAjvb5pX"
      },
      "outputs": [],
      "source": [
        "class CNNGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNGenerator, self).__init__()\n",
        "        self.w = nn.Linear(latent_dim, hidden_size * 2 * 4 * 4, bias=False)\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.BatchNorm2d(hidden_size * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(hidden_size * 2, hidden_size, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_size),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( hidden_size, hidden_size // 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_size // 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d( hidden_size // 2, hidden_size // 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_size // 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 32 x 32\n",
        "            nn.ConvTranspose2d( hidden_size // 4, 3, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = self.w(input).view((-1, hidden_size * 2, 4, 4))\n",
        "        return self.main(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1Bz035Ugroo"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wY_LoVXOgtao"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=4, stride_size=4, emb_size=384, image_size=32, batch_size=64):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.projection = nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains\n",
        "            spectral_norm(nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=stride_size)).to(device),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        num_patches = ((image_size-patch_size+stride_size) // stride_size) **2 + 1\n",
        "        self.positions = nn.Parameter(torch.randn(num_patches, emb_size))\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.projection(x)\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        # prepend the cls token to the input\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        # add position embedding\n",
        "        x += torch.sin(self.positions)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8DZy_pOg0ID"
      },
      "outputs": [],
      "source": [
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        \n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkfQ73pSgwQo"
      },
      "outputs": [],
      "source": [
        "class DiscriminatorTransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size=384,\n",
        "                 drop_p=0.,\n",
        "                 forward_expansion=4,\n",
        "                 forward_drop_p=0.,\n",
        "                 **kwargs):\n",
        "        super().__init__(\n",
        "                ResidualAdd(nn.Sequential(\n",
        "                    nn.LayerNorm(emb_size),\n",
        "                    MultiHeadAttention(emb_size, **kwargs),\n",
        "                    nn.Dropout(drop_p)\n",
        "                )),\n",
        "                ResidualAdd(nn.Sequential(\n",
        "                    nn.LayerNorm(emb_size),\n",
        "                    nn.Sequential(\n",
        "                        spectral_norm(nn.Linear(emb_size, forward_expansion * emb_size)),\n",
        "                        nn.GELU(),\n",
        "                        nn.Dropout(forward_drop_p),\n",
        "                        spectral_norm(nn.Linear(forward_expansion * emb_size, emb_size)),\n",
        "                    ),\n",
        "                    nn.Dropout(drop_p)\n",
        "                )\n",
        "            ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6IYQuoNg9EA"
      },
      "outputs": [],
      "source": [
        "class DiscriminatorTransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth=4, **kwargs):\n",
        "        super().__init__(*[DiscriminatorTransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
        "\n",
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size=384, class_size_1=4098, class_size_2=1024, class_size_3=512, n_classes=10):\n",
        "        super().__init__(\n",
        "            nn.LayerNorm(emb_size),\n",
        "            spectral_norm(nn.Linear(emb_size, class_size_1)),\n",
        "            nn.GELU(),\n",
        "            spectral_norm(nn.Linear(class_size_1, class_size_2)),\n",
        "            nn.GELU(),\n",
        "            spectral_norm(nn.Linear(class_size_2, class_size_3)),\n",
        "            nn.GELU(),\n",
        "            spectral_norm(nn.Linear(class_size_3, n_classes)),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Take only the cls token outputs\n",
        "        x = x[:, 0, :]\n",
        "        return super().forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oALpYkMhCGV"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Sequential):\n",
        "    def __init__(self,     \n",
        "                in_channels=3,\n",
        "                patch_size=4,\n",
        "                stride_size=4,\n",
        "                emb_size=384,\n",
        "                image_size=32,\n",
        "                depth=4,\n",
        "                n_classes=1,\n",
        "                diffaugment='color,translation,cutout',\n",
        "                **kwargs):\n",
        "        self.diffaugment = diffaugment\n",
        "        super().__init__(\n",
        "            PatchEmbedding(in_channels, patch_size, stride_size, emb_size, image_size),\n",
        "            DiscriminatorTransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
        "            ClassificationHead(emb_size, n_classes=n_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, img, do_augment=True):\n",
        "        if do_augment:\n",
        "            img = DiffAugment(img, policy=self.diffaugment)\n",
        "        return super().forward(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu4HNFgpaLZ2"
      },
      "source": [
        "# CNN Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyj4W0kQLJ2j"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                diffaugment='color,translation,cutout',\n",
        "                **kwargs):\n",
        "        self.diffaugment = diffaugment\n",
        "        super().__init__(\n",
        "            nn.Conv2d(3,32,kernel_size=3,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32,64,kernel_size=3,stride=1,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "\n",
        "            nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128,128,kernel_size=3,stride=1,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "\n",
        "            nn.Conv2d(128,256,kernel_size=3,stride=1,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256,256,kernel_size=3,stride=1,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256*4*4,1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024,512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, img, do_augment=True):\n",
        "        if do_augment:\n",
        "            img = DiffAugment(img, policy=self.diffaugment)\n",
        "        return super().forward(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbniFkThhR5T"
      },
      "source": [
        "# StyleGAN2 Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW_m-p4pg34m"
      },
      "outputs": [],
      "source": [
        "class StyleGanDiscriminator(stylegan2_pytorch.Discriminator):\n",
        "    def __init__(self,\n",
        "                diffaugment='color,translation,cutout',\n",
        "                **kwargs):\n",
        "        self.diffaugment = diffaugment\n",
        "        super().__init__(**kwargs)\n",
        "    def forward(self, img, do_augment=True):\n",
        "        if do_augment:\n",
        "            img = DiffAugment(img, policy=self.diffaugment)\n",
        "        out, _ = super().forward(img)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsKPASQ3XtSU"
      },
      "source": [
        "# Diversity Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB52QuhSWhRk"
      },
      "outputs": [],
      "source": [
        "def diversity_loss(images):\n",
        "    num_images_to_calculate_on = 10\n",
        "    num_pairs = num_images_to_calculate_on * (num_images_to_calculate_on - 1) // 2\n",
        "\n",
        "    scale_factor = 5\n",
        "\n",
        "    loss = torch.zeros(1, dtype=torch.float, device=device, requires_grad=True)\n",
        "    i = 0\n",
        "    for a_id in range(num_images_to_calculate_on):\n",
        "        for b_id in range(a_id+1, num_images_to_calculate_on):\n",
        "            img_a = images[a_id]\n",
        "            img_b = images[b_id]\n",
        "            img_a_l2 = torch.norm(img_a)\n",
        "            img_b_l2 = torch.norm(img_b)\n",
        "            img_a, img_b = img_a.flatten(), img_b.flatten()\n",
        "\n",
        "            # print(img_a_l2, img_b_l2, img_a.shape, img_b.shape)\n",
        "\n",
        "            a_b_loss = scale_factor * (img_a.t() @ img_b) / (img_a_l2 * img_b_l2)\n",
        "            # print(a_b_loss)\n",
        "            loss = loss + torch.sigmoid(a_b_loss)\n",
        "            i += 1\n",
        "    loss = loss.sum() / num_pairs\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFkNgPe5J4WZ"
      },
      "source": [
        "# Normal distribution init weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg23vO0CGoPr"
      },
      "outputs": [],
      "source": [
        "def init_normal(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        if 'weight' in m.__dict__.keys():\n",
        "            m.weight.data.normal_(0.0,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Akp8Ybo483q"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS5sfmIuLj8P"
      },
      "outputs": [],
      "source": [
        "if generator_type == \"vitgan\":\n",
        "    # Create the Generator\n",
        "    Generator = GeneratorViT(   patch_size=patch_size,\n",
        "                                image_size=image_size,\n",
        "                                style_mlp_layers=style_mlp_layers,\n",
        "                                latent_dim=latent_dim,\n",
        "                                hidden_size=hidden_size,\n",
        "                                combine_patch_embeddings=combine_patch_embeddings,\n",
        "                                combined_embedding_size=combined_embedding_size,\n",
        "                                sln_paremeter_size=sln_paremeter_size,\n",
        "                                num_heads=num_heads,\n",
        "                                depth=depth,\n",
        "                                forward_drop_p=dropout_p,\n",
        "                                bias=bias,\n",
        "                                weight_modulation=weight_modulation,\n",
        "                                siren_hidden_layers=siren_hidden_layers,\n",
        "                                demodulation=demodulation,\n",
        "                            ).to(device)\n",
        "                            \n",
        "    # use the modules apply function to recursively apply the initialization\n",
        "    Generator.apply(init_normal)\n",
        "\n",
        "    num_patches_x = int(image_size//out_patch_size)\n",
        "\n",
        "    if os.path.exists(f'{experiment_folder_name}/weights/Generator.pth'):\n",
        "        Generator = torch.load(f'{experiment_folder_name}/weights/Generator.pth')\n",
        "\n",
        "    wandb.watch(Generator)\n",
        "\n",
        "elif generator_type == \"cnn\":\n",
        "    cnn_generator = CNNGenerator().to(device)\n",
        "\n",
        "    cnn_generator.apply(init_normal)\n",
        "\n",
        "    if os.path.exists(f'{experiment_folder_name}/weights/cnn_generator.pth'):\n",
        "        cnn_generator = torch.load(f'{experiment_folder_name}/weights/cnn_generator.pth')\n",
        "\n",
        "    wandb.watch(cnn_generator)\n",
        "\n",
        "# Create the three types of discriminators\n",
        "if discriminator_type == \"vitgan\":\n",
        "    Discriminator = ViT(discriminator=True,\n",
        "                            patch_size=patch_size*2,\n",
        "                            stride_size=patch_size,\n",
        "                            n_classes=1, \n",
        "                            num_heads=num_heads,\n",
        "                            depth=depth,\n",
        "                            forward_drop_p=dropout_p,\n",
        "                    ).to(device)\n",
        "            \n",
        "    Discriminator.apply(init_normal)\n",
        "    \n",
        "    if os.path.exists(f'{experiment_folder_name}/weights/discriminator.pth'):\n",
        "        Discriminator = torch.load(f'{experiment_folder_name}/weights/discriminator.pth')\n",
        "\n",
        "    wandb.watch(Discriminator)\n",
        "\n",
        "elif discriminator_type == \"cnn\":\n",
        "    cnn_discriminator = CNN().to(device)\n",
        "\n",
        "    cnn_discriminator.apply(init_normal)\n",
        "\n",
        "    if os.path.exists(f'{experiment_folder_name}/weights/discriminator.pth'):\n",
        "        cnn_discriminator = torch.load(f'{experiment_folder_name}/weights/discriminator.pth')\n",
        "\n",
        "    wandb.watch(cnn_discriminator)\n",
        "\n",
        "elif discriminator_type == \"stylegan2\":\n",
        "    stylegan2_discriminator = StyleGanDiscriminator(image_size=32).to(device)\n",
        "\n",
        "    # stylegan2_discriminator.apply(init_normal)\n",
        "\n",
        "    if os.path.exists(f'{experiment_folder_name}/weights/discriminator.pth'):\n",
        "        stylegan2_discriminator = torch.load(f'{experiment_folder_name}/weights/discriminator.pth')\n",
        "\n",
        "    wandb.watch(stylegan2_discriminator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9EdXO0K8qFW"
      },
      "source": [
        "# Testing the generator\n",
        "\n",
        "Train to match fixed latent values to fixed images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BsgyaZMMi2f"
      },
      "outputs": [],
      "source": [
        "total_steps = 0 # Since the whole image is our dataset, this just means 500 gradient descent steps.\n",
        "steps_til_summary = 50\n",
        "\n",
        "if generator_type == \"vitgan\":\n",
        "    params = Generator.parameters()\n",
        "else:\n",
        "    # z = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
        "    params = list(cnn_generator.parameters())\n",
        "optim = torch.optim.Adam(lr=lr, params=params)\n",
        "ema = ExponentialMovingAverage(params, decay=0.995)\n",
        "\n",
        "ground_truth, _ = next(iter(trainloader))\n",
        "ground_truth = ground_truth.permute(0, 2, 3, 1).view((-1, image_size**2, out_features))\n",
        "ground_truth = ground_truth.to(device)\n",
        "\n",
        "z = torch.randn([batch_size, latent_dim]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Swv4ZQt94833"
      },
      "outputs": [],
      "source": [
        "for step in range(total_steps):\n",
        "    if generator_type == \"vitgan\":\n",
        "        model_output = Generator(z)\n",
        "    elif generator_type == \"cnn\":\n",
        "        model_output = cnn_generator(z)\n",
        "        model_output = model_output.permute([0, 2, 3, 1]).view([-1, image_size**2, out_features])\n",
        "    loss = ((model_output - ground_truth)**2).mean()\n",
        "    \n",
        "    if not step % steps_til_summary:\n",
        "        print(\"Step %d, Total loss %0.6f\" % (step, loss))\n",
        "\n",
        "        fig, axes = plt.subplots(2,8, figsize=(24,6))\n",
        "        for i in range(8):\n",
        "            j = np.random.randint(0, batch_size-1)\n",
        "            img = model_output[j].cpu().view(32,32,3).detach().numpy()\n",
        "            img -= img.min()\n",
        "            img /= img.max()\n",
        "            axes[0,i].imshow(img)\n",
        "            g_img = ground_truth[j].cpu().view(32,32,3).detach().numpy()\n",
        "            g_img -= g_img.min()\n",
        "            g_img /= g_img.max()\n",
        "            axes[1,i].imshow(g_img)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    ema.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcWiPCochjJ7"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad_UXnQJCOeN"
      },
      "outputs": [],
      "source": [
        "os.makedirs(f\"{experiment_folder_name}/weights\", exist_ok = True)\n",
        "os.makedirs(f\"{experiment_folder_name}/samples\", exist_ok = True)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "if discriminator_type == \"cnn\": discriminator = cnn_discriminator\n",
        "elif discriminator_type == \"stylegan2\": discriminator = stylegan2_discriminator\n",
        "elif discriminator_type == \"vitgan\": discriminator = Discriminator\n",
        "\n",
        "if generator_type == \"cnn\":\n",
        "    params = cnn_generator.parameters()\n",
        "else:\n",
        "    params = Generator.parameters()\n",
        "optim_g = torch.optim.Adam(lr=lr, params=params, betas=beta)\n",
        "optim_d = torch.optim.Adam(lr=lr_dis, params=discriminator.parameters(), betas=beta)\n",
        "ema = ExponentialMovingAverage(params, decay=0.995)\n",
        "\n",
        "fixed_noise = torch.FloatTensor(np.random.normal(0, 1, (16, latent_dim))).to(device)\n",
        "\n",
        "discriminator_f_img = torch.zeros([batch_size, 3, image_size, image_size]).to(device)\n",
        "\n",
        "trainset_len = len(trainloader.dataset)\n",
        "\n",
        "step = 0\n",
        "for epoch in range(epochs):\n",
        "    for batch_id, batch in enumerate(trainloader):\n",
        "        step += 1\n",
        "\n",
        "        # Train discriminator\n",
        "\n",
        "        # Forward + Backward with real images\n",
        "        r_img = batch[0].to(device)\n",
        "        r_logit = discriminator(r_img).flatten()\n",
        "        r_label = torch.ones(r_logit.shape[0]).to(device)\n",
        "\n",
        "        lossD_real = criterion(r_logit, r_label)\n",
        "        \n",
        "        lossD_bCR_real = F.mse_loss(r_logit, discriminator(r_img, do_augment=False))\n",
        "\n",
        "        # Forward + Backward with fake images\n",
        "        latent_vector = torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))).to(device)\n",
        "\n",
        "        if generator_type == \"vitgan\":\n",
        "            f_img = Generator(latent_vector)\n",
        "            f_img = f_img.reshape([-1, image_size, image_size, out_features])\n",
        "            f_img = f_img.permute(0, 3, 1, 2)\n",
        "        else:\n",
        "            model_output = cnn_generator(latent_vector)\n",
        "            f_img = model_output\n",
        "            \n",
        "        assert f_img.size(0) == batch_size, f_img.shape\n",
        "        assert f_img.size(1) == out_features, f_img.shape\n",
        "        assert f_img.size(2) == image_size, f_img.shape\n",
        "        assert f_img.size(3) == image_size, f_img.shape\n",
        "\n",
        "        f_label = torch.zeros(batch_size).to(device)\n",
        "        # Save the a single generated image to the discriminator training data\n",
        "        if batch_size_history_discriminator:\n",
        "            discriminator_f_img[step % batch_size] = f_img[0].detach()\n",
        "            f_logit_history = discriminator(discriminator_f_img).flatten()\n",
        "            lossD_fake_history = criterion(f_logit_history, f_label)\n",
        "        else: lossD_fake_history = 0\n",
        "        # Train the discriminator on the images, generated only from this batch\n",
        "        f_logit = discriminator(f_img.detach()).flatten()\n",
        "        lossD_fake = criterion(f_logit, f_label)\n",
        "        \n",
        "        lossD_bCR_fake = F.mse_loss(f_logit, discriminator(f_img, do_augment=False))\n",
        "        \n",
        "        f_noise_input = torch.FloatTensor(np.random.rand(*f_img.shape)*2 - 1).to(device)\n",
        "        f_noise_logit = discriminator(f_noise_input).flatten()\n",
        "        lossD_noise = criterion(f_noise_logit, f_label)\n",
        "\n",
        "        lossD = lossD_real * 0.5 +\\\n",
        "                lossD_fake * 0.5 +\\\n",
        "                lossD_fake_history * lambda_lossD_history +\\\n",
        "                lossD_noise * lambda_lossD_noise +\\\n",
        "                lossD_bCR_real * lambda_bCR_real +\\\n",
        "                lossD_bCR_fake * lambda_bCR_fake\n",
        "\n",
        "        optim_d.zero_grad()\n",
        "        lossD.backward()\n",
        "        optim_d.step()\n",
        "        \n",
        "        # Train Generator\n",
        "\n",
        "        if generator_type == \"vitgan\":\n",
        "            f_img = Generator(latent_vector)\n",
        "            f_img = f_img.reshape([-1, image_size, image_size, out_features])\n",
        "            f_img = f_img.permute(0, 3, 1, 2)\n",
        "        else:\n",
        "            model_output = cnn_generator(latent_vector)\n",
        "            f_img = model_output\n",
        "        \n",
        "        assert f_img.size(0) == batch_size\n",
        "        assert f_img.size(1) == out_features\n",
        "        assert f_img.size(2) == image_size\n",
        "        assert f_img.size(3) == image_size\n",
        "\n",
        "        f_logit = discriminator(f_img).flatten()\n",
        "        r_label = torch.ones(batch_size).to(device)\n",
        "        lossG_main = criterion(f_logit, r_label)\n",
        "        \n",
        "        lossG_diversity = diversity_loss(f_img) * lambda_diversity_penalty\n",
        "        lossG = lossG_main + lossG_diversity\n",
        "        \n",
        "        optim_g.zero_grad()\n",
        "        lossG.backward()\n",
        "        optim_g.step()\n",
        "        ema.update()\n",
        "\n",
        "        writer.add_scalar(\"Loss/Generator\", lossG_main, step)\n",
        "        writer.add_scalar(\"Loss/Gen(diversity)\", lossG_diversity, step)\n",
        "        writer.add_scalar(\"Loss/Dis(real)\", lossD_real, step)\n",
        "        writer.add_scalar(\"Loss/Dis(fake)\", lossD_fake, step)\n",
        "        writer.add_scalar(\"Loss/Dis(fake_history)\", lossD_fake_history, step)\n",
        "        writer.add_scalar(\"Loss/Dis(noise)\", lossD_noise, step)\n",
        "        writer.add_scalar(\"Loss/Dis(bCR_fake)\", lossD_bCR_fake * lambda_bCR_fake, step)\n",
        "        writer.add_scalar(\"Loss/Dis(bCR_real)\", lossD_bCR_real * lambda_bCR_real, step)\n",
        "        writer.flush()\n",
        "\n",
        "        wandb.log({\n",
        "            'Generator': lossG_main,\n",
        "            'Gen(diversity)': lossG_diversity,\n",
        "            'Dis(real)': lossD_real,\n",
        "            'Dis(fake)': lossD_fake,\n",
        "            'Dis(fake_history)': lossD_fake_history,\n",
        "            'Dis(noise)': lossD_noise,\n",
        "            'Dis(bCR_fake)': lossD_bCR_fake * lambda_bCR_fake,\n",
        "            'Dis(bCR_real)': lossD_bCR_real * lambda_bCR_real\n",
        "        })\n",
        "\n",
        "        if batch_id%20 == 0:\n",
        "            print(f'epoch {epoch}/{epochs}; batch {batch_id}/{int(trainset_len/batch_size)}')\n",
        "            print(f'Generator: {\"{:.3f}\".format(float(lossG_main))}, '+\\\n",
        "                  f'Gen(diversity): {\"{:.3f}\".format(float(lossG_diversity))}, '+\\\n",
        "                  f'Dis(real): {\"{:.3f}\".format(float(lossD_real))}, '+\\\n",
        "                  f'Dis(fake): {\"{:.3f}\".format(float(lossD_fake))}, '+\\\n",
        "                  f'Dis(fake_history): {\"{:.3f}\".format(float(lossD_fake_history))}, '+\\\n",
        "                  f'Dis(noise) {\"{:.3f}\".format(float(lossD_noise))}, '+\\\n",
        "                  f'Dis(bCR_fake): {\"{:.3f}\".format(float(lossD_bCR_fake * lambda_bCR_fake))}, '+\\\n",
        "                  f'Dis(bCR_real): {\"{:.3f}\".format(float(lossD_bCR_real * lambda_bCR_real))}')\n",
        "\n",
        "            # Plot 8 randomly selected samples\n",
        "            fig, axes = plt.subplots(1,8, figsize=(24,3))\n",
        "            output = f_img.permute(0, 2, 3, 1)\n",
        "            for i in range(8):\n",
        "                j = np.random.randint(0, batch_size-1)\n",
        "                img = output[j].cpu().view(32,32,3).detach().numpy()\n",
        "                img -= img.min()\n",
        "                img /= img.max()\n",
        "                axes[i].imshow(img)\n",
        "            plt.show()\n",
        "\n",
        "    # if step % sample_interval == 0:\n",
        "    if generator_type == \"vitgan\":\n",
        "        Generator.eval()\n",
        "        vis = Generator(fixed_noise)\n",
        "        vis = vis.reshape([-1, image_size, image_size, out_features])\n",
        "        vis = vis.permute(0, 3, 1, 2)\n",
        "    else:\n",
        "        model_output = cnn_generator(fixed_noise)\n",
        "        vis = model_output\n",
        "\n",
        "    assert vis.shape[0] == fixed_noise.shape[0], f'vis.shape[0] is {vis.shape[0]}, but should be {fixed_noise.shape[0]}'\n",
        "    assert vis.shape[1] == out_features, f'vis.shape[1] is {vis.shape[1]}, but should be {out_features}'\n",
        "    assert vis.shape[2] == image_size, f'vis.shape[2] is {vis.shape[2]}, but should be {image_size}'\n",
        "    assert vis.shape[3] == image_size, f'vis.shape[3] is {vis.shape[3]}, but should be {image_size}'\n",
        "    \n",
        "    vis.detach().cpu()\n",
        "    vis = make_grid(vis, nrow = 4, padding = 5, normalize = True)\n",
        "    writer.add_image(f'Generated/epoch_{epoch}', vis)\n",
        "    wandb.log({'examples': wandb.Image(vis)})\n",
        "\n",
        "    vis = T.ToPILImage()(vis)\n",
        "    vis.save(f'{experiment_folder_name}/samples/vis{epoch}.jpg')\n",
        "    if generator_type == \"vitgan\":\n",
        "        Generator.train()\n",
        "    else:\n",
        "        cnn_generator.train()\n",
        "    print(f\"Save sample to {experiment_folder_name}/samples/vis{epoch}.jpg\")\n",
        "\n",
        "    # Save the checkpoints.\n",
        "    if generator_type == \"vitgan\":\n",
        "        torch.save(Generator, f'{experiment_folder_name}/weights/Generator.pth')\n",
        "    elif generator_type == \"cnn\":\n",
        "        torch.save(cnn_generator, f'{experiment_folder_name}/weights/cnn_generator.pth')\n",
        "    torch.save(discriminator, f'{experiment_folder_name}/weights/discriminator.pth')\n",
        "    print(\"Save model state.\")\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWDnspPtsot5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ViTGAN-pytorch.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
